{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d79a7da-8525-475d-a2ee-c1df9732c0df",
   "metadata": {},
   "source": [
    "# Exercise 6: Data Cleaning \n",
    "1. **Inspecting the columns of the DataFrame:**\n",
    "   We'll use the `info()` method to get an overview of the DataFrame's columns, their data types, and the number of non-null values in each column.\n",
    "\n",
    "```python\n",
    "df.info()\n",
    "```\n",
    "\n",
    "2. **Inspecting rows with bad values in the price column:**\n",
    "   We can filter the DataFrame to show only the rows where the price column has the value 'price'.\n",
    "\n",
    "```python\n",
    "bad_price_rows = df[df['price'] == 'price']\n",
    "print(bad_price_rows)\n",
    "```\n",
    "\n",
    "3. **Dropping rows with accidental column names:**\n",
    "   If the column names have been fed into the data in intervals, we'll need to identify these rows and drop them.\n",
    "\n",
    "```python\n",
    "# Assuming the DataFrame is named df\n",
    "df = df[df['company'] != 'company']\n",
    "```\n",
    "\n",
    "4. **Converting price column to the appropriate type:**\n",
    "   We'll convert the price column to a numeric data type, such as float.\n",
    "\n",
    "```python\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "```\n",
    "\n",
    "Since the 'price' column should contain numerical values, we should check for rows where the 'price' column contains non-numeric values. Let's adjust the approach for step 2 accordingly:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2 adaptation: Inspect rows with bad values in the price column\n",
    "bad_price_rows = df[~df['price'].astype(str).str.isnumeric()]\n",
    "print(bad_price_rows)\n",
    "\n",
    "```\n",
    "\n",
    "This adjusted code will identify rows where the 'price' column contains non-numeric values and proceed accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66536d8c-108b-4c34-bd6e-e9125286a0b1",
   "metadata": {},
   "source": [
    "# On pd.to_numeric()\n",
    "The `pd.to_numeric()` function in pandas is used to convert a column of a DataFrame into numeric format. It can handle various types of input data and convert them into numeric types like integer or float. Here's an explanation of the parameters, including the `errors` parameter:\n",
    "\n",
    "- **`errors`**: This parameter specifies how errors should be handled. It accepts three possible values:\n",
    "  - `'raise'` (default): If any error occurs during conversion, it raises an exception.\n",
    "  - `'coerce'`: If any error occurs, it returns `NaN` (Not a Number) for that particular value.\n",
    "  - `'ignore'`: It simply ignores errors and leaves the original values unchanged.\n",
    "\n",
    "Here's how the `errors` parameter affects the conversion process:\n",
    "\n",
    "- **`errors='raise'`**: This setting is the default behavior. If any value cannot be converted to a numeric type, it raises an error, stopping the conversion process. This is useful when you want to ensure that all values are successfully converted and want to be notified if there are any issues.\n",
    "\n",
    "- **`errors='coerce'`**: This setting is useful when you want to convert the column to numeric type but are okay with missing values (`NaN`) for those entries that cannot be converted. It replaces the problematic values with `NaN`, allowing the conversion process to continue for the rest of the data.\n",
    "\n",
    "- **`errors='ignore'`**: This setting is useful when you want to convert as many values as possible to numeric type without raising any errors. It simply skips the problematic values and leaves them unchanged in the resulting output.\n",
    "\n",
    "Here's an example of how you might use `pd.to_numeric()` with different error handling:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'numeric_column': ['1', '2', '3', 'four', '5']}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'numeric_column' to numeric, coercing errors\n",
    "df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "   numeric_column\n",
    "0             1.0\n",
    "1             2.0\n",
    "2             3.0\n",
    "3             NaN\n",
    "4             5.0\n",
    "```\n",
    "\n",
    "In this example, the string 'four' couldn't be converted to a numeric type. With `errors='coerce'`, it replaced 'four' with `NaN`, allowing the conversion to proceed for the other values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee9d348-a8cd-41e3-8e41-b6a1c833b515",
   "metadata": {},
   "source": [
    "# Missing values\n",
    "\n",
    "1. To identify missing values in the DataFrame, you can use the `isnull()` method along with the `sum()` method to count the missing values in each column.\n",
    "2. You can then identify which columns contain missing values by looking at the counts obtained in step 1.\n",
    "3. Inspecting rows containing missing values can be done using the `head()` method to display the first few rows or using boolean indexing to filter rows with missing values.\n",
    "4. Dropping rows with missing values across specific columns can be achieved using the `dropna()` method with the `subset` parameter specifying the columns to consider.\n",
    "5. The code `df[['vehicle_class', 'fare', 'price']].groupby(['vehicle_class', 'fare']).mean()` calculates the mean price for each combination of vehicle class and fare.\n",
    "6. Filling missing price values with the mean of all prices can be done using the `fillna()` method.\n",
    "7. Finally, you can check if all NaN values have been removed by again using the `isnull()` method and `sum()` method to count missing values.\n",
    "\n",
    "Let's implement these steps:\n",
    "\n",
    "```python\n",
    "# Step 1: Identify missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Step 2: Identify columns with missing values\n",
    "columns_with_missing_values = missing_values[missing_values > 0].index.tolist()\n",
    "\n",
    "# Step 3: Inspect rows with missing values\n",
    "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Step 4: Drop rows with missing values across specific columns\n",
    "df_cleaned = df.dropna(subset=['vehicle_class', 'price', 'fare'])\n",
    "\n",
    "# Step 5: Analyze ticket price with respect to vehicle_class and fare\n",
    "price_analysis = df_cleaned[['vehicle_class', 'fare', 'price']].groupby(['vehicle_class', 'fare']).mean()\n",
    "\n",
    "# Step 6: Fill missing price values with the mean of all prices\n",
    "mean_price = df_cleaned['price'].mean()\n",
    "df_cleaned['price'] = df_cleaned['price'].fillna(mean_price)\n",
    "\n",
    "# Step 7: Check for remaining missing values\n",
    "remaining_missing_values = df_cleaned.isnull().sum()\n",
    "\n",
    "# Display results\n",
    "print(\"Step 1: Missing Values\\n\", missing_values)\n",
    "print(\"\\nStep 2: Columns with Missing Values\\n\", columns_with_missing_values)\n",
    "print(\"\\nStep 3: Rows with Missing Values\\n\", rows_with_missing_values)\n",
    "print(\"\\nStep 4: Cleaned DataFrame (Rows with Missing Values Dropped)\\n\", df_cleaned)\n",
    "print(\"\\nStep 5: Price Analysis\\n\", price_analysis)\n",
    "print(\"\\nStep 6: Missing Values after Filling\\n\", remaining_missing_values)\n",
    "```\n",
    "\n",
    "This code will provide you with the necessary insights and actions to handle missing values in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79296d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85948 entries, 0 to 85947\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   company        85948 non-null  object\n",
      " 1   origin         85948 non-null  object\n",
      " 2   destination    85948 non-null  object\n",
      " 3   departure      85948 non-null  object\n",
      " 4   arrival        85948 non-null  object\n",
      " 5   vehicle_class  77116 non-null  object\n",
      " 6   price          72769 non-null  object\n",
      " 7   fare           77116 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(\"slide_data/renfe_trains.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a576adc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price\n",
       "price    3875\n",
       "76.3     3794\n",
       "85.1     3615\n",
       "107.7    2845\n",
       "53.4     2719\n",
       "         ... \n",
       "98.01       1\n",
       "98.2        1\n",
       "69.05       1\n",
       "19.75       1\n",
       "61.15       1\n",
       "Name: count, Length: 389, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8384e5",
   "metadata": {},
   "source": [
    "Seems like there is some bad values in the price column with the value 'price'. Lets inspect the rows where this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22bf0b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>departure</th>\n",
       "      <th>arrival</th>\n",
       "      <th>vehicle_class</th>\n",
       "      <th>price</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85903</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85908</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85921</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85934</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85942</th>\n",
       "      <td>company</td>\n",
       "      <td>origin</td>\n",
       "      <td>destination</td>\n",
       "      <td>departure</td>\n",
       "      <td>arrival</td>\n",
       "      <td>vehicle_class</td>\n",
       "      <td>price</td>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3875 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       company  origin  destination  departure  arrival  vehicle_class  price  \\\n",
       "69     company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "146    company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "209    company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "287    company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "347    company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "...        ...     ...          ...        ...      ...            ...    ...   \n",
       "85903  company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "85908  company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "85921  company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "85934  company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "85942  company  origin  destination  departure  arrival  vehicle_class  price   \n",
       "\n",
       "       fare  \n",
       "69     fare  \n",
       "146    fare  \n",
       "209    fare  \n",
       "287    fare  \n",
       "347    fare  \n",
       "...     ...  \n",
       "85903  fare  \n",
       "85908  fare  \n",
       "85921  fare  \n",
       "85934  fare  \n",
       "85942  fare  \n",
       "\n",
       "[3875 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['price']=='price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885d644",
   "metadata": {},
   "source": [
    "It looks like some sort of error has meant the column names have been fed into the data in intervals. Let's drop these rows as they are clearly an accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc92a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['price'] != 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43342cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price\n",
       "76.3     3794\n",
       "85.1     3615\n",
       "107.7    2845\n",
       "53.4     2719\n",
       "60.3     2544\n",
       "         ... \n",
       "49.67       1\n",
       "166.6       1\n",
       "69.39       1\n",
       "55.85       1\n",
       "61.15       1\n",
       "Name: count, Length: 388, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249b8ca",
   "metadata": {},
   "source": [
    "We can now represent price as having the appropriate type. Convert it so that it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "133e9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28691ab9-e994-4155-b656-4fddcd80bbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company              0\n",
      "origin               0\n",
      "destination          0\n",
      "departure            0\n",
      "arrival              0\n",
      "vehicle_class     8832\n",
      "price            13179\n",
      "fare              8832\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify missing values\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"slide_data/renfe_trains.csv\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ece3b-5523-40fa-aa24-aa406a5c3d31",
   "metadata": {},
   "source": [
    "In the line `missing_values = df.isnull().sum()`, several operations are happening:\n",
    "\n",
    "1. `df.isnull()` generates a DataFrame of the same shape as `df` where each cell contains a boolean value indicating whether the corresponding cell in `df` is null (missing) or not.\n",
    "2. `.sum()` is then called on this boolean DataFrame. This operation sums up the boolean values along each column axis, effectively counting the number of missing values in each column.\n",
    "3. The result is a pandas Series where the index represents the column names of the DataFrame `df`, and the values represent the count of missing values in each column.\n",
    "4. Finally, this Series is assigned to the variable `missing_values`.\n",
    "\n",
    "So, `missing_values` is a pandas Series containing the count of missing values in each column of the DataFrame `df`. The index of the Series corresponds to the column names, and the values represent the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae9cc64e-e6df-4f9e-b396-31e3c1f7a698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vehicle_class', 'price', 'fare']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Identify columns with missing values\n",
    "columns_with_missing_values = missing_values[missing_values > 0].index.tolist()\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a6845-11db-4133-aa6f-148ad66a216e",
   "metadata": {},
   "source": [
    "Let's break down the code `columns_with_missing_values = missing_values[missing_values > 0].index.tolist()` step by step:\n",
    "\n",
    "1. `missing_values[missing_values > 0]`: This part selects only those entries from the `missing_values` Series where the count of missing values is greater than 0. In other words, it filters out columns that have at least one missing value. This operation returns a new Series containing only those entries where missing values are present, along with their corresponding counts.\n",
    "\n",
    "2. `.index`: This part accesses the index of the filtered Series obtained in the previous step. Since we filtered out columns with missing values, the index now represents the column names that have missing values.\n",
    "\n",
    "3. `.tolist()`: Finally, the `.tolist()` method is used to convert the index of the filtered Series into a Python list. This list contains the names of columns that have missing values.\n",
    "\n",
    "So, `columns_with_missing_values` is a Python list containing the names of columns from the DataFrame `df` that have missing values. This list is obtained by filtering the `missing_values` Series to include only those columns with counts of missing values greater than 0, and then extracting their column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "990ad558-c809-4277-a938-17e9c41b547f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      company  origin destination            departure              arrival  \\\n",
      "11      renfe  MADRID   BARCELONA  2019-05-03 18:30:00  2019-05-03 21:20:00   \n",
      "15      renfe  MADRID   BARCELONA  2019-04-23 07:30:00  2019-04-23 10:40:00   \n",
      "33      renfe  MADRID     SEVILLA  2019-04-21 21:25:00  2019-04-22 00:10:00   \n",
      "52      renfe  MADRID     SEVILLA  2019-04-17 09:45:00  2019-04-17 12:27:00   \n",
      "65      renfe  MADRID     SEVILLA  2019-05-03 13:30:00  2019-05-03 16:05:00   \n",
      "...       ...     ...         ...                  ...                  ...   \n",
      "85847   renfe  MADRID     SEVILLA  2020-11-22 09:00:00  2020-11-22 11:37:48   \n",
      "85850   renfe  MADRID     SEVILLA  2020-10-13 11:22:00  2020-10-13 16:05:12   \n",
      "85854   renfe  MADRID   BARCELONA  2020-11-06 10:30:00  2020-11-06 13:15:00   \n",
      "85866   renfe  MADRID     SEVILLA  2020-12-04 12:00:00  2020-12-04 14:31:48   \n",
      "85871   renfe  MADRID     SEVILLA  2020-10-13 11:22:00  2020-10-13 16:05:12   \n",
      "\n",
      "      vehicle_class price      fare  \n",
      "11       Preferente   NaN     Promo  \n",
      "15          Turista   NaN     Promo  \n",
      "33              NaN   NaN       NaN  \n",
      "52          Turista   NaN  Flexible  \n",
      "65          Turista   NaN     Promo  \n",
      "...             ...   ...       ...  \n",
      "85847           NaN   NaN       NaN  \n",
      "85850           NaN   NaN       NaN  \n",
      "85854           NaN   NaN       NaN  \n",
      "85866           NaN   NaN       NaN  \n",
      "85871           NaN   NaN       NaN  \n",
      "\n",
      "[13179 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Inspect rows with missing values\n",
    "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
    "print(rows_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca4519-bcee-4679-b2d7-8c049222a9cb",
   "metadata": {},
   "source": [
    "### 3 in depth\n",
    "Let's delve into step 3, which is `rows_with_missing_values = df[df.isnull().any(axis=1)]`, and break it down:\n",
    "\n",
    "1. `df.isnull()`: This part creates a DataFrame of the same shape as `df` (the original DataFrame) where each entry is either True or False depending on whether the corresponding entry in `df` is null (missing) or not null (not missing).\n",
    "\n",
    "2. `.any(axis=1)`: This part applies the `.any()` method along axis 1, which corresponds to rows. It checks each row of the DataFrame obtained in step 1 and returns True if any of the entries in that row is True (indicating a missing value), and False otherwise. So, for each row, it returns True if there's at least one missing value in that row, and False if there are no missing values in that row.\n",
    "\n",
    "3. `df[...]`: This part uses boolean indexing to select rows from the original DataFrame `df`. It selects rows where the condition specified inside the square brackets is True. In this case, the condition is the result of `df.isnull().any(axis=1)`, which identifies rows that contain at least one missing value.\n",
    "\n",
    "Putting it all together, `rows_with_missing_values` is a DataFrame containing only the rows from the original DataFrame `df` that have at least one missing value. This step effectively filters the DataFrame to retain only those rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a379d66f-c8d8-49d5-9d06-cf1fc5a0d5fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      company  origin destination            departure              arrival  \\\n",
      "0       renfe  MADRID     SEVILLA  2019-05-07 19:00:00  2019-05-07 21:38:00   \n",
      "1       renfe  MADRID     SEVILLA  2019-05-07 21:25:00  2019-05-08 00:10:00   \n",
      "2       renfe  MADRID   BARCELONA  2019-05-11 08:30:00  2019-05-11 11:15:00   \n",
      "3       renfe  MADRID   BARCELONA  2019-04-18 14:30:00  2019-04-18 17:21:00   \n",
      "4       renfe  MADRID   BARCELONA  2019-04-28 16:30:00  2019-04-28 19:15:00   \n",
      "...       ...     ...         ...                  ...                  ...   \n",
      "85943   renfe  MADRID   BARCELONA  2020-10-20 08:00:00  2020-10-20 10:30:00   \n",
      "85944   renfe  MADRID   BARCELONA  2020-10-27 09:30:00  2020-10-27 12:34:12   \n",
      "85945   renfe  MADRID   BARCELONA  2020-10-23 07:30:00  2020-10-23 10:40:12   \n",
      "85946   renfe  MADRID   BARCELONA  2020-10-08 07:40:00  2020-10-08 13:55:00   \n",
      "85947   renfe  MADRID   BARCELONA  2020-11-08 16:30:00  2020-11-08 19:15:00   \n",
      "\n",
      "            vehicle_class  price      fare  \n",
      "0              Preferente   69.4     Promo  \n",
      "1                 Turista  43.55     Promo  \n",
      "2                 Turista   85.1     Promo  \n",
      "3                 Turista  107.7  Flexible  \n",
      "4                 Turista  107.7  Flexible  \n",
      "...                   ...    ...       ...  \n",
      "85943             Turista  55.25   Promo +  \n",
      "85944             Turista  55.55   Promo +  \n",
      "85945             Turista  46.85   Promo +  \n",
      "85946  Turista con enlace   96.7  Flexible  \n",
      "85947             Turista  108.9  Flexible  \n",
      "\n",
      "[72769 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Drop rows with missing values across specific columns\n",
    "df_cleaned = df.dropna(subset=['vehicle_class', 'price', 'fare'])\n",
    "\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb51031-bb94-4f4d-9719-ff11c39751b8",
   "metadata": {},
   "source": [
    "### Step 4 in depth\n",
    "\n",
    "Step 4 involves dropping rows from the DataFrame `df` where there are missing values across specific columns, namely 'vehicle_class', 'price', and 'fare'. Let's break down this step in detail:\n",
    "\n",
    "1. `df.dropna(subset=['vehicle_class', 'price', 'fare'])`: This method drops rows from the DataFrame `df` where there are missing values (NaN) in any of the specified columns. The `subset` parameter specifies the columns to consider when determining if a row should be dropped. In this case, it specifies 'vehicle_class', 'price', and 'fare'.\n",
    "\n",
    "2. The `dropna()` method, when used with the `subset` parameter, drops rows only if there are missing values in the specified subset of columns. If any of the columns specified in the `subset` parameter have missing values for a particular row, that row will be removed from the DataFrame.\n",
    "\n",
    "3. The result of this operation is assigned to a new DataFrame named `df_cleaned`, which now contains only the rows from the original DataFrame `df` that do not have any missing values in the specified columns ('vehicle_class', 'price', and 'fare').\n",
    "\n",
    "In summary, Step 4 ensures that `df_cleaned` contains only those rows from the original DataFrame `df` that have non-missing values in the specified columns, effectively removing rows with missing values in any of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4eab101-dc8a-4487-ae04-9fdc464f2f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85948 entries, 0 to 85947\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   company        85948 non-null  object\n",
      " 1   origin         85948 non-null  object\n",
      " 2   destination    85948 non-null  object\n",
      " 3   departure      85948 non-null  object\n",
      " 4   arrival        85948 non-null  object\n",
      " 5   vehicle_class  77116 non-null  object\n",
      " 6   price          72769 non-null  object\n",
      " 7   fare           77116 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c205a8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af72a17c-b4de-4366-aa89-830c0efc10b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1874\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[1;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1874\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39magg_series(ser, alt, preserve_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1875\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:849\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    847\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 849\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_series_pure_python(obj, func)\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:877\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[1;32m--> 877\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group)\n\u001b[0;32m    878\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2380\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   2377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[0;32m   2379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 2380\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[0;32m   2381\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[0;32m   2382\u001b[0m     )\n\u001b[0;32m   2383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6225\u001b[0m, in \u001b[0;36mSeries.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m   6217\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   6218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m   6219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6223\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   6224\u001b[0m ):\n\u001b[1;32m-> 6225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:11992\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  11986\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11987\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11990\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11991\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 11992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  11993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  11994\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:11949\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11947\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m> 11949\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[0;32m  11950\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  11951\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6133\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   6129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   6130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6132\u001b[0m     )\n\u001b[1;32m-> 6133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[1;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/renfe_trains.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 5: Analyze ticket price with respect to vehicle_class and fare\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      6\u001b[0m price_analysis \u001b[38;5;241m=\u001b[39m df_cleaned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(price_analysis)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2378\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[1;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   2371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[0;32m   2372\u001b[0m         grouped_mean,\n\u001b[0;32m   2373\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[0;32m   2374\u001b[0m         engine_kwargs,\n\u001b[0;32m   2375\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2376\u001b[0m     )\n\u001b[0;32m   2377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[0;32m   2379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2380\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[0;32m   2381\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[0;32m   2382\u001b[0m     )\n\u001b[0;32m   2383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1929\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1929\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgrouped_reduce(array_func)\n\u001b[0;32m   1930\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[0;32m   1931\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1428\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[1;32m-> 1428\u001b[0m         applied \u001b[38;5;241m=\u001b[39m sb\u001b[38;5;241m.\u001b[39mapply(func)\n\u001b[0;32m   1429\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:366\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    368\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1926\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1926\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[0;32m   1927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1878\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[1;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1877\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[1;32m-> 1878\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m   1881\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": [
    "# Step 5: Analyze ticket price with respect to vehicle_class and fare\n",
    "\n",
    "df[['vehicle_class', 'fare', 'price']].groupby(['vehicle_class', 'fare']).mean()\n",
    "price_analysis = df_cleaned[['vehicle_class', 'fare', 'price']].groupby(['vehicle_class', 'fare']).mean()\n",
    "print(price_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cca963-10b9-4bd3-ad7e-6a0188e30e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
